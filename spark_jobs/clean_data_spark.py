import os
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lower
from pyspark.sql.types import IntegerType, DoubleType

# Validate import pandera
try:
    import pandera as pa
    from pandera.typing import DataFrame, Series
except ImportError:
    pa = None

# --- 1. CONFIGURATION & UTILS ---


def setup_windows_env(base_dir):
    """C·∫•u h√¨nh bi·∫øn m√¥i tr∆∞·ªùng cho Spark tr√™n Windows"""
    # Hadoop
    hadoop_path = os.path.join(base_dir, 'bin', 'hadoop')
    os.environ['HADOOP_HOME'] = hadoop_path

    if not os.path.exists(os.path.join(hadoop_path, 'bin', 'winutils.exe')):
        # Ch·ªâ warning ch·ª© kh√¥ng exit, ƒë·ªÉ launcher.py c√≥ th·ªÉ x·ª≠ l√Ω logic Linux
        print(f"‚ö†Ô∏è Warning: Kh√¥ng t√¨m th·∫•y winutils.exe t·∫°i {hadoop_path}")

    # Java (Auto-detect)
    adoptium_dir = os.path.join(base_dir, 'bin', 'Eclipse Adoptium')
    try:
        if os.path.exists(adoptium_dir):
            jdk_name = [f for f in os.listdir(
                adoptium_dir) if f.startswith('jdk-11')][0]
            java_path = os.path.join(adoptium_dir, jdk_name)
            os.environ['JAVA_HOME'] = java_path
    except Exception:
        print("‚ö†Ô∏è Warning: Kh√¥ng t√¨m th·∫•y JDK 11 local. D√πng Java h·ªá th·ªëng.")

    # Update Path
    paths = [os.path.join(hadoop_path, 'bin')]
    if 'JAVA_HOME' in os.environ:
        paths.append(os.path.join(os.environ['JAVA_HOME'], 'bin'))

    os.environ['PATH'] = os.pathsep.join(paths + [os.environ['PATH']])


def get_paths():
    """
    Tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n S3 (MinIO) ƒë·ªÉ Spark x·ª≠ l√Ω.
    """
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

    # ƒê∆Ø·ªúNG D·∫™N MINIO (S3A)
    # ƒê·∫£m b·∫£o b·∫°n ƒë√£ upload file telco_churn.parquet l√™n bucket datalake/raw
    input_path = "s3a://datalake/raw/telco_churn.parquet"
    output_path = "s3a://datalake/processed/features"

    # QUAN TR·ªåNG: Ph·∫£i c√≥ d√≤ng return n√†y m·ªõi s·ª≠a ƒë∆∞·ª£c l·ªói NoneType
    return base_dir, input_path, output_path

# --- 2. TRANSFORMATION LOGIC ---


def clean_dataframe(df):
    """H√†m ch·ª©a to√†n b·ªô logic l√†m s·∫°ch d·ªØ li·ªáu"""

    # A. Chu·∫©n h√≥a t√™n c·ªôt
    df = df.select([col(c).alias(c.strip().lower().replace(' ', ''))
                   for c in df.columns])
    cols = df.columns

    # B. X·ª≠ l√Ω c·ªôt Churn
    if 'churnvalue' in cols:
        df = df.withColumn("Churn", col("churnvalue").cast(IntegerType()))
    elif 'churnlabel' in cols:
        df = df.withColumn("Churn", when(
            lower(col("churnlabel")) == "yes", 1).otherwise(0))
    elif 'churn' in cols:
        df = df.withColumn("Churn", when(
            col("churn").isin("Yes", "1"), 1).otherwise(0))

    df = df.fillna(0, subset=["Churn"])

    # C. √âp ki·ªÉu s·ªë
    numeric_cols = {'totalcharges': 'TotalCharges',
                    'monthlycharges': 'MonthlyCharges'}
    for src, dest in numeric_cols.items():
        if src in cols:
            df = df.withColumn(dest, col(src).cast(
                DoubleType())).fillna(0.0, subset=[dest])

    # D. X·ª≠ l√Ω Tenure
    tenure_col = 'tenuremonths' if 'tenuremonths' in cols else (
        'tenure' if 'tenure' in cols else None)
    if tenure_col:
        df = df.withColumn("tenure", col(tenure_col).cast(IntegerType()))

    # E. ƒê·ªïi t√™n ID
    if 'customerid' in cols:
        df = df.withColumnRenamed("customerid", "customerID")

    # F. Ch·ªçn l·ªçc c·ªôt
    required = ['customerID', 'tenure',
                'MonthlyCharges', 'TotalCharges', 'Churn']
    final_cols = [c for c in required if c in df.columns]

    # T·∫°o bi·∫øn k·∫øt qu·∫£ TR∆Ø·ªöC khi validate
    df_result = df.select(*final_cols)

    # G. Validate Data
    if pa:
        try:
            print("üîç Validating data schema...")
            sample_pd = df_result.limit(5).toPandas()
            schema = pa.DataFrameSchema({
                "MonthlyCharges": pa.Column(float, checks=pa.Check.ge(0), required=False),
                "Churn": pa.Column(int, checks=pa.Check.isin([0, 1]), required=False)
            })
            schema.validate(sample_pd)
            print("‚úÖ Data Validation Passed!")
        except Exception as e:
            print(f"‚ö†Ô∏è Validation Warning: {e}")

    return df_result

# --- 3. MAIN EXECUTION ---


def run():
    base_dir, input_path, output_path = get_paths()
    setup_windows_env(base_dir)

    print("üîå Configuring Spark for MinIO/S3...")

    # C·∫•u h√¨nh Spark + AWS Jars ƒë·ªÉ n·ªëi MinIO
    spark = SparkSession.builder \
        .appName("CDP_Telco_ETL") \
        .master("local[*]") \
        .config("spark.sql.caseSensitive", "false") \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "admin") \
        .config("spark.hadoop.fs.s3a.secret.key", "password") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")

    try:
        print(f"üöÄ Reading from MinIO: {input_path}")
        df = spark.read.parquet(input_path)

        df_clean = clean_dataframe(df)

        print(f"üíæ Writing to MinIO: {output_path}")
        df_clean.coalesce(1).write.mode("overwrite").parquet(output_path)
        print("‚úÖ SUCCESS! Data saved to Data Lake (MinIO).")

    except Exception as e:
        print(f"‚ùå ERROR: {e}")
    finally:
        spark.stop()


if __name__ == "__main__":
    run()
